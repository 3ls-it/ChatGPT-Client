#!/usr/bin/env python3 
"""
  You will need to choose a ChatGPT model that works with
this API, not all do.
https://platform.openai.com/docs/models

-J Adams jfa63[at]duck[dot]com April 2025
"""

import os
import readline
import sys

import openai
from openai import OpenAI
from openai import AssistantEventHandler

from rich import print
from rich.console import Console
from rich.markdown import Markdown

from typing_extensions import override
from prompt_toolkit import PromptSession
from prompt_toolkit.key_binding import KeyBindings

homedir = os.path.expanduser("~") + "/"
os.makedirs(homedir + ".openai", exist_ok=True)
sys.path.insert(0, homedir + ".openai")
from settings import (TEMP,
                      MODEL,
                      YOUR_NAME,
                      ASSISTANT_NAME,
                      OPENAI_API_KEY)


gpt_model = MODEL
temp = TEMP
your_name = YOUR_NAME
assistant_name = ASSISTANT_NAME
api_key = OPENAI_API_KEY

# Open log file in append mode 
logfile = homedir + ".openai/chatgpt_log.txt"
lf = open(logfile, 'a', encoding='utf-8')

# Create a PromptSession object
session = PromptSession(multiline=True)

# Define custom key bindings
kb = KeyBindings()

# Create a Rich Console for response display
console = Console()


# Define a key combination (e.g., Tab+Enter) to submit input
@kb.add('tab', 'enter')
def _(event):
    event.app.current_buffer.validate_and_handle()
# End _() 


def file_upload(client, vector_store, filename: str) -> bool :
    # Ready the files for upload to OpenAI 
    # filenames relative to homedir. 
    file_paths = [filename]
    file_streams = [open(path, "rb") for path in file_paths]

    # Use the upload and poll SDK helper to upload the files,
    # add them to the vector store,
    # then poll the status of the file batch for completion.
    file_batch = client.vector_stores.file_batches.upload_and_poll(
            vector_store_id=vector_store.id, files=file_streams
    )

    # Close the file streams  
    for fs in file_streams:
        fs.close()

    # Print the status and the file counts of the batch.
    print(f"Uploaded: [blue]{filename}[/blue]\n"+file_batch.status)
    #print(file_batch.file_counts)
    #print("")
    return True
# End file_upload()  


# Extend the EventHandler class overriding two methods   
# to handle the events in the response stream. 
# Added text attribute 
class EventHandler(AssistantEventHandler):    
    text: str
    
    def __init__(self):
        # Initialize handler to set up internal state
        super().__init__()
        # Accumulate streamed text here
        self.text = ''

    @override
    def on_text_created(self, text) -> None:
        # Write to logfile. 
        lf.write(f'{assistant_name}: ')
        print(f'\n[#f5d676]{assistant_name}:[/#f5d676] ', end='', flush=True)
    
    @override
    def on_text_delta(self, delta, snapshot):
        resp = str(delta.value)
        # Write delta to logfile. 
        lf.write(resp)
        # Concatenate deltas for display. 
        self.text += resp
    
    def on_tool_call_created(self, tool_call):
        print(f'\n> {tool_call.type}\n', flush=True)

    def on_tool_call_delta(self, delta, snapshot):
        if delta.type == 'code_interpreter':
            if delta.code_interpreter.input:
                print(delta.code_interpreter.input, end='', flush=True)
            if delta.code_interpreter.outputs:
                print('\n\nOutput >', flush=True)
                for output in delta.code_interpreter.outputs:
                    if output.type == 'logs':
                        print(f'\n{output.logs}', flush=True)
# End class EventHandler() 



def main():
    # Clear terminal 
    os.system('clear')
    print("[bold blue]Uploading chat log file...[/bold blue]")

    # Instantiate an OpenAI object. 
    try:
        # Use this if your key is in settings.py 
        client = OpenAI(api_key=api_key)
        # Use this if you export your key as an evironmental variable. 
        #client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])

        # Create and configure an Assistant object. 
        assistant = client.beta.assistants.create(
                instructions = f"You are {assistant_name}, {your_name}'s personal AI assistant. All coding and programming queries refer to Unix-like environments unless otherwise noted. Where possible, provide citations",
                name = assistant_name,
                tools = [{"type": "code_interpreter"},
                         {"type": "file_search"}],
                         model = gpt_model,
                         temperature = temp
        )

        # Here we set up a Vector Store to hold the chat history.
        vector_store = client.vector_stores.create(name="Chat History")
        file_upload(client, vector_store, logfile)

        # Update Assistant object 
        assistant = client.beta.assistants.update(
                assistant_id=assistant.id,
                tool_resources={"file_search":
                                {"vector_store_ids": [vector_store.id]}},
        ) 
    
        # Create Thread instance. 
        thread = client.beta.threads.create()

    except openai.APIConnectionError as e:
        print("The server could not be reached")
        print(e)  # an underlying Exception, likely raised within httpx.
        sys.exit(1)
    except openai.AuthenticationError as e:
        print("Check your API key or token and make sure it is correct and active.")
        print(e)  # an underlying Exception, likely raised within httpx.
        sys.exit(1)
    except Exception as e:
        print(e)
        sys.exit(1)

    # We take user input, create a message,  
    # create a Run, then stream the whole of the response. 
    print(f'[#f5d676]\nHow can I help you {your_name}?\n[/#f5d676]')
    while True:
        # Take user input. 
        try:
            # Get user input with custom key bindings
            query = session.prompt('>> ', key_bindings=kb)
        except (EOFError, KeyboardInterrupt):
            break

        if query == "Done":
            print(f'[#f5d676]Bye! -{assistant_name}[/#f5d676]')
            break

        if query == "Upload":
            fname = input('Filename realative to ~/: ')
            if os.path.isfile(homedir + fname) is True:
                print("You have chosen the file:")
                print(homedir + fname)
                resp = input("Is this correct? [Y/n] ")
                # Handle typical "yes" responses 
                if resp.lower() in ("y", "yes", ""):
                    try:
                        file_upload(client, vector_store, homedir+fname)
                        # Update Assistant object 
                        assistant = client.beta.assistants.update(
                              assistant_id=assistant.id,
                              tool_resources={"file_search":
                                             {"vector_store_ids": [vector_store.id]}},
                        )
                        # Prevent sending "Upload" as a chat message
                        continue
                    except openai.BadRequestError as e:
                        print("Recieved a BadRequestError from OpenAI.")
                        print("This could mean that the file type is unsupported.\n")
                        print(e)
                        continue
                else:
                    print("[red]Cancelled[/red]\n\n")
                    continue
            else:
                print("The file:")
                print(homedir + fname)
                print("does not exist, has been moved, or is a directory.\n\n")
                continue

        try:
            # Write the query to logfile. 
            quest = "Me: " + query
            lf.write(quest+"\n\n")

            # Create a message from the user query. 
            client.beta.threads.messages.create(
                    thread_id=thread.id,
                    role="user",
                    content=query
            )

            # Then, we use the `stream` SDK helper 
            # with the `EventHandler` class to create the Run 
            # and stream the response. We then render and display
            # the response using Console() and Markdown() from rich. 
            with client.beta.threads.runs.stream(
                    thread_id=thread.id,
                    assistant_id=assistant.id,
                    event_handler=EventHandler(),
                ) as stream:
                        stream.until_done()
                        md = Markdown(stream.text)
                        console.print(md)
                        lf.write("\n")
                        print("\n")

        except openai.APIConnectionError as e:
            print("The server could not be reached")
            print(e)  # an underlying Exception, likely raised within httpx.
        except openai.RateLimitError as e:
            print("A 429 status code was received; we should back off a bit.")
            print(e)
        except openai.APIStatusError as e:
            print("A non-200-range status code was received")
            print(e.status_code)
            print(e.response)
        # End try 
    # End While  

    lf.close()
    print()
# End main 


if __name__ == '__main__':
    main()
