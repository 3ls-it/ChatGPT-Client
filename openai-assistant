#!/usr/bin/env python3 
"""
Copyright 2024, 2025 J Adams jfa63[at]duck[dot]com May 2025
Released under the 2-Clause BSD license.
"""
# Builtin imports 

import os
import sys
import traceback
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

# OpenAI imports 
import openai
from openai import OpenAI
from openai import AssistantEventHandler

# Rich imports for output formatting 
from rich import print
from rich.console import Console
from rich.markdown import Markdown

# Imports for prompt inputs 
from typing_extensions import override
from prompt_toolkit import PromptSession
from prompt_toolkit.key_binding import KeyBindings

# Imports for configuration loading  
import runpy
from pathlib import Path


## Some global definitions 
__version__ = '1.0.2'
# Define home directory and config paths
homedir     = str(Path.home()) + "/"
CONFIG_DIR  = Path(homedir) / ".openai"
CONFIG_PATH = CONFIG_DIR / "settings.py"
logfile = homedir + ".openai/chat_log.txt"
# Create a PromptSession object
session = PromptSession(multiline=True)
# Define custom key bindings
kb = KeyBindings()
# Create a Rich Console for response display
console = Console()

# Required and optional settings 
REQUIRED  = ("MODEL", "TEMP", "YOUR_NAME", "ASSISTANT_NAME", "OPENAI_API_KEY")
DEFAULTS  = {"MODEL": "gpt-4.1", "TEMP": 0.3, "YOUR_NAME": "User", "ASSISTANT_NAME": "OpenAI"}
OPTIONAL  = ("FURTHER_INSTRUCTIONS", "VECTOR_STORE_ID", "ASSISTANT_ID")



## Helper functions 
# Helper to load configuration settings 
def load_settings():
    if CONFIG_PATH.is_file():
        return runpy.run_path(str(CONFIG_PATH))
    return {}
# End load_settings() 


# Helper to update a single key in ~/.openai/settings.py
def write_setting(key: str, val):
    lines = []
    found = False
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        for line in f:
            if line.startswith(f"{key} ="):
                # overwrite the existing line
                if isinstance(val, str):
                    lines.append(f"{key} = '{val}'\n")
                else:
                    lines.append(f"{key} = {val}\n")
                found = True
            else:
                lines.append(line)
    if not found:
        # append at end
        if isinstance(val, str):
            lines.append(f"{key} = '{val}'\n")
        else:
            lines.append(f"{key} = {val}\n")
    with open(CONFIG_PATH, "w", encoding="utf-8") as f:
        f.writelines(lines)
# End write_settings() 


# Define a key combination (e.g., Tab+Enter) to submit input
@kb.add('tab', 'enter')
def _(event):
    event.app.current_buffer.validate_and_handle()
# End _() 


# Upload file 
# Upload and then trim the vector store so only the latest log file remains
def file_upload(client, vector_store, filename: str) -> bool:
    # Skip if the file is empty or missing
    if not os.path.isfile(filename) or os.path.getsize(filename) == 0:
        print(f"Skipping empty file upload: {filename}")
        return False
    file_paths = [filename]
    file_streams = [open(path, "rb") for path in file_paths]

    # Upload files to the vector store
    file_batch = client.vector_stores.file_batches.upload_and_poll(
        vector_store_id=vector_store.id, files=file_streams
    )

    for fs in file_streams:
        fs.close()

    print(f"Uploaded: [blue]{filename}[/blue]\n" + file_batch.status)

    # --- Trimming logic: delete all files from vector store except this latest log file ---
    file_list = client.vector_stores.files.list(vector_store_id=vector_store.id)
    # Find the uploaded file by filename (match display name if possible)
    keep_ids = set()
    log_basename = os.path.basename(filename)
    for f in file_list:
        # Some SDKs use .filename, some .display_name, so check both
        display_name = getattr(f, "display_name", None) or getattr(f, "filename", None)
        if display_name and os.path.basename(display_name) == log_basename:
            keep_ids.add(f.id)

    for f in file_list:
        if f.id not in keep_ids:
            client.vector_stores.files.delete(vector_store_id=vector_store.id, file_id=f.id)
            print(f"[yellow]Trimmed file from vector store:[/yellow] {f.id}")
    return True
# End file_upload()  



# Extend the EventHandler class overriding two methods   
# to handle the events in the response stream. 
# Added text attribute 
class EventHandler(AssistantEventHandler):    
    text: str
    
    def __init__(self):
        # Initialize handler to set up internal state
        super().__init__()
        # Accumulate streamed text here
        self.text = ''

    @override
    def on_text_created(self, text) -> None:
        # Write to logfile. 
        lf.write(f'{assistant_name}: ')
        print(f'\n[#f5d676]{assistant_name}:[/#f5d676] ', end='', flush=True)
    
    @override
    def on_text_delta(self, delta, snapshot):
        resp = str(delta.value)
        # Write delta to logfile. 
        lf.write(resp)
        # Concatenate deltas for display. 
        self.text += resp
    
    def on_tool_call_created(self, tool_call):
        print(f'\n> {tool_call.type}\n', flush=True)

    def on_tool_call_delta(self, delta, snapshot):
        if delta.type == 'code_interpreter':
            if delta.code_interpreter.input:
                print(delta.code_interpreter.input, end='', flush=True)
            if delta.code_interpreter.outputs:
                print('\n\nOutput >', flush=True)
                for output in delta.code_interpreter.outputs:
                    if output.type == 'logs':
                        print(f'\n{output.logs}', flush=True)
# End class EventHandler() 



## Get and load  settings, prompt for missing settings  
settings = load_settings()
updated  = False

# Prompt for missing required settings
for key in REQUIRED:
    if key not in settings or (key == "OPENAI_API_KEY" and not settings.get(key)):
        if key in DEFAULTS:
            settings[key] = DEFAULTS[key]
            print(f"No {key} found. Using default: {settings[key]}")
        else:
            settings[key] = input(f"Enter {key}: ").strip()
        updated = True

# Ensure optional settings exist
for key in OPTIONAL:
    if key not in settings:
        settings[key] = ""
        updated = True

# Write settings file if needed
if updated or not CONFIG_PATH.exists():
    CONFIG_DIR.mkdir(parents=True, exist_ok=True)
    with open(CONFIG_PATH, "w", encoding="utf-8") as f:
        f.write("# ~/.openai/settings.py  (auto-generated by openai-assistant)\n")
        for key in REQUIRED + OPTIONAL:
            val = settings[key]
            if isinstance(val, str):
                f.write(f"{key} = '{val}'\n")
            else:
                f.write(f"{key} = {val}\n")
    print(f"Wrote settings to {CONFIG_PATH}")

# Expose settings as module-level names
MODEL                = settings["MODEL"]
TEMP                 = settings["TEMP"]
OPENAI_API_KEY       = settings["OPENAI_API_KEY"]
YOUR_NAME            = settings["YOUR_NAME"]
ASSISTANT_NAME       = settings["ASSISTANT_NAME"]
FURTHER_INSTRUCTIONS = settings["FURTHER_INSTRUCTIONS"]
VECTOR_STORE_ID      = settings["VECTOR_STORE_ID"]
ASSISTANT_ID         = settings["ASSISTANT_ID"]

gpt_model = MODEL
temp = TEMP
your_name = YOUR_NAME
assistant_name = ASSISTANT_NAME
api_key = OPENAI_API_KEY
instructions = f"You are {assistant_name}, {your_name}'s personal AI assistant."

# Open log file in append mode 
lf = open(logfile, 'a', encoding='utf-8')



def main():
    # Clear terminal 
    os.system('clear')

    # Instantiate an OpenAI object. 
    try:
        client = OpenAI(api_key=api_key)
        # Set up or retrieve an Assistant to handle requests.
        if ASSISTANT_ID:
            assistant = client.beta.assistants.retrieve(
                assistant_id=ASSISTANT_ID
            )
        else:
            assistant = client.beta.assistants.create(
                instructions=instructions + FURTHER_INSTRUCTIONS,
                name=assistant_name,
                tools=[{"type": "code_interpreter"}, {"type": "file_search"}],
                model=gpt_model,
                temperature=temp
            )
            print(f"[bold green]Created new assistant with ID {assistant.id}[/bold green]")
            # Offer to write ASSISTANT_ID back to settings.py
            resp = input("Save ASSISTANT_ID to settings.py? [Y/n] ").strip().lower()
            if resp in ("", "y", "yes"):
                write_setting("ASSISTANT_ID", assistant.id)
                print(f"Updated ASSISTANT_ID in {CONFIG_PATH}")
            else:
                print("ASSISTANT_ID not saved; will create new next run.")

        # Set up or retrieve a Vector Store to hold the chat history.
        if VECTOR_STORE_ID:
            # Retrieve existing vector store by ID
            vector_store = client.vector_stores.retrieve(vector_store_id=VECTOR_STORE_ID)
        else:
            vector_store = client.vector_stores.create(name="Chat History")
            print(f"[bold green]Created new vector store with ID {vector_store.id}[/bold green]")
            # Offer to write VECTOR_STORE_ID back to settings.py
            resp = input("Save VECTOR_STORE_ID to settings.py? [Y/n] ").strip().lower()
            if resp in ("", "y", "yes"):
                write_setting("VECTOR_STORE_ID", vector_store.id)
                print(f"Updated VECTOR_STORE_ID in {CONFIG_PATH}")
            else:
                print("VECTOR_STORE_ID not saved; will create new next run.")

        # Upload chat log 
        print("[bold blue]Uploading chat log file...[/bold blue]")
        file_upload(client, vector_store, logfile)

        # Update Assistant object with vector store
        assistant = client.beta.assistants.update(
                assistant_id=assistant.id,
                tool_resources={"file_search":
                                {"vector_store_ids": [vector_store.id]}},
        )
    
        # Create Thread instance. 
        thread = client.beta.threads.create()

    except openai.APIConnectionError as e:
        print("The server could not be reached")
        print(e)  # an underlying Exception, likely raised within httpx.
        sys.exit(1)
    except openai.AuthenticationError as e:
        print("Check your API key or token and make sure it is correct and active.")
        print(e)  # an underlying Exception, likely raised within httpx.
        sys.exit(1)
    except Exception as e:
        print(f"Unexpected error: {e}")
        traceback.print_exc()
        sys.exit(1)

    # We take user input, create a message,  
    # create a Run, then stream the whole of the response. 
    print('[bold green]Enter "Done" or "done" to quit session\n[/bold green]')
    print(f'[#f5d676]\nHow can I help you {your_name}?\n[/#f5d676]')
    while True:
        # Take user input. 
        try:
            # Get user input with custom key bindings
            query = session.prompt('>> ', key_bindings=kb)
        except (EOFError, KeyboardInterrupt):
            break

        if query in ("done", "done\n", "Done", "Done\n"):
            print(f'[#f5d676]Bye! -{assistant_name}[/#f5d676]')
            break

        if query == "Upload":
            fname = input('Filename realative to ~/: ')
            if os.path.isfile(homedir + fname) is True:
                print("You have chosen the file:")
                print(homedir + fname)
                resp = input("Is this correct? [Y/n] ")
                # Handle typical "yes" responses 
                if resp.lower() in ("y", "yes", ""):
                    try:
                        file_upload(client, vector_store, homedir+fname)
                        # Update Assistant object 
                        assistant = client.beta.assistants.update(
                              assistant_id=assistant.id,
                              tool_resources={"file_search":
                                             {"vector_store_ids": [vector_store.id]}},
                        )
                        # Prevent sending "Upload" as a chat message
                        continue
                    except openai.BadRequestError as e:
                        print("Recieved a BadRequestError from OpenAI.")
                        print("This could mean that the file type is unsupported.\n")
                        print(e)
                        continue
                else:
                    print("[red]Cancelled[/red]\n\n")
                    continue
            else:
                print("The file:")
                print(homedir + fname)
                print("does not exist, has been moved, or is a directory.\n\n")
                continue

        try:
            # Write the query to logfile. 
            quest = "\n\nMe: " + query
            lf.write(quest+"\n")

            # Create a message from the user query. 
            client.beta.threads.messages.create(
                    thread_id=thread.id,
                    role="user",
                    content=query
            )

            # Then, we use the `stream` SDK helper 
            # with the `EventHandler` class to create the Run 
            # and stream the response. We then render and display
            # the response using Console() and Markdown() from rich. 
            with client.beta.threads.runs.stream(
                    thread_id=thread.id,
                    assistant_id=assistant.id,
                    event_handler=EventHandler(),
                ) as stream:
                        stream.until_done()
                        md = Markdown(stream.text)
                        console.print(md)
                        lf.write("\n")
                        print("\n")

        except openai.APIConnectionError as e:
            print("The server could not be reached")
            print(e)  # an underlying Exception, likely raised within httpx.
        except openai.RateLimitError as e:
            print("A 429 status code was received; we should back off a bit.")
            print(e)
        except openai.APIStatusError as e:
            print("A non-200-range status code was received")
            print(e.status_code)
            print(e.response)
        # End try 
    # End While  

    lf.close()
    print()
# End main()  


if __name__ == '__main__':
    main()
